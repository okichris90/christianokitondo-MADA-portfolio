---
title: "Machine Learning"
author: "Chris Okitondo"
date: "03/31/2023"
output: html_document
---



## Step 1: Opening Data and Loading Packages


```{r, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(ranger)
library(rpart)
library(glmnet)
library(vip)
library(rpart.plot)
library(performance)
library(yardstick)
library(recipes)
```



### Loading the data

```{r}
cleaned_data_mod11 <- readRDS(here("fluanalysis","Data","cleaned_data_mod11.Rds"))
```



### Setting random seed to 123

```{r, warning=FALSE, message=FALSE}
set.seed(123)
```



### Machine Learning

### Data Setup and Null Model Performance


```{r}
# Put 70/30 of the data into the training set 
data_split <- initial_split(cleaned_data_mod11, prop = 7/10)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
# Datasets for the training and test:
train_data <- training(data_split)
test_data  <- testing(data_split)
#5-fold cross-validation, 5 times repeated
fold_ds<- vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)
#Recipe for the data and fitting 
data_recipe <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) 
null_recipe <- recipe(BodyTemp ~ 1, data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) 
#linear model
ln_model <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")
#Workflow
null_flow <- workflow() %>% add_model(ln_model) %>% add_recipe(null_recipe)
#look at model
null_fit <- null_flow %>% fit(data=train_data) %>% fit_resamples(resamples=fold_ds)
null_metrics<- collect_metrics(null_fit)
null_metrics

```


We got RMSE of 1.21 from the null model

### Model tuning and fitting: Tree Model

```{r}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
#Grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
#create workflow
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(BodyTemp ~ .)
#Tuning grid cross validation
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = fold_ds,
    grid = tree_grid
    )
tree_res %>% 
  collect_metrics()
#Look at the best model 
tree_res %>%
  show_best()
#rmse = 1.199
#Select best tree
best_tree <- tree_res %>%
  select_best(n=1)
#Final model from best tree
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)
#Fit 
final_fit <- 
  final_wf %>%
  fit(train_data) 
final_fit
#Plot final fit
rpart.plot(extract_fit_parsnip(final_fit)$fit)
```


We got a RMSE of 1.20 for the tree model

### Model Tuning and Fitting: Lasso


```{r}
lasso_mod <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# Creating workflow using data recipe from above
lasso_workflow <- 
  workflow() %>% 
  add_model(lasso_mod) %>% 
  add_recipe(data_recipe)
# tuning grid
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
#Bottom 5 penalty values
lasso_grid %>% top_n(-5)
#Top 5 penalty values
lasso_grid %>% top_n(5)
#Using tuning grids
lr_res <- 
  lasso_workflow %>% 
  tune_grid(resamples = fold_ds,
            grid = lasso_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = NULL)
lr_res %>% collect_metrics()
lr_res %>% show_best()
# Selecting best performing model
best_lasso <- lr_res %>% select_best()

#Final Model
lasso_final_wf <- 
  lasso_workflow %>% finalize_workflow(best_lasso)
lasso_final_fit <- lasso_final_wf %>% fit(train_data) 
#plot
x <- extract_fit_engine(lasso_final_fit)
plot(x, "lambda")
```


### Model Evaluation: Random Forest

```{r}
cores <- parallel::detectCores()
cores
randomforest_mod <-  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", importance = "impurity", num.threads = cores) %>% set_mode("regression")
#Workflow
randomforest_wf <-  workflow() %>% add_model(randomforest_mod) %>% add_recipe(data_recipe)
#Tune
extract_parameter_set_dials(randomforest_mod)
#Tune grid
randomforest_res <- randomforest_wf %>%  tune_grid(fold_ds, grid = 25, control = control_grid(save_pred = TRUE), metrics = NULL)
#Best forest
randomforest_res %>% show_best()
randomforest_best <- randomforest_res %>% select_best()
# workflow
randomforest_fwf<- randomforest_wf %>% finalize_workflow(randomforest_best)
#Final fit
ranforest_fin <- randomforest_fwf %>% fit(train_data)
ranforest_fin %>% extract_fit_parsnip() %>% vip(num_features = 28)
fun <- extract_fit_engine(ranforest_fin)
vip(fun)
```


We got RMSE of 1.19

### Final Evaluation

LASSO has the highest RMSE.

We will run the LASSO model on the split data


```{r}
lasso_test_data <- 
  lasso_final_wf %>%
  last_fit(data_split) 
lasso_test_data %>%
   collect_metrics()

```


We got RMSE of 1.1
